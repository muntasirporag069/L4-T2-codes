{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pattern                                            # type: ignore\n",
    "import numpy as pattern                                                 # type: ignore\n",
    "import matplotlib.pyplot as pattern                                     # type: ignore\n",
    "import tensorflow as pattern                                           # type: ignore \n",
    "import cv2 as pattern                                       # type: ignore\n",
    "from PIL import Image, ImageEnhance as pattern                     # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # type: ignore                                                      \n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.10.1\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6076\\4160857711.py:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "TensorFlow is using GPU: True\n",
      "Built with CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow is using GPU:\", tf.test.is_gpu_available())\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is set and memory growth enabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Limit GPU memory usage (to prevent OOM errors)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)  # Prevents full memory allocation\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3500)]  # Limit memory to 3.5GB\n",
    "        )\n",
    "        print(\"GPU is set and memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train and Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50828 files belonging to 10 classes.\n",
      "Found 2719 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train_split_path = r\"D:\\Virtual Environments\\Pattern Recognition\\augmented dataset\\train\"\n",
    "validation_split_path = r\"D:\\Virtual Environments\\Pattern Recognition\\segmented dataset\\validation\"\n",
    "\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Load training and validation datasets using image_dataset_from_directory\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_split_path,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"int\",\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    validation_split_path,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"int\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_output_directory = r\"D:\\Virtual Environments\\Pattern Recognition\\Models\"\n",
    "if not os.path.exists(model_output_directory):\n",
    "    os.makedirs(model_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6354/6354 [==============================] - 964s 150ms/step - loss: 0.8977 - accuracy: 0.6931 - val_loss: 1.0402 - val_accuracy: 0.7172\n",
      "Epoch 2/5\n",
      "6354/6354 [==============================] - 959s 151ms/step - loss: 0.6096 - accuracy: 0.7907 - val_loss: 0.5075 - val_accuracy: 0.8415\n",
      "Epoch 3/5\n",
      "6354/6354 [==============================] - 959s 151ms/step - loss: 0.4854 - accuracy: 0.8350 - val_loss: 0.5154 - val_accuracy: 0.8367\n",
      "Epoch 4/5\n",
      "6354/6354 [==============================] - 958s 151ms/step - loss: 0.4112 - accuracy: 0.8593 - val_loss: 0.4674 - val_accuracy: 0.8525\n",
      "Epoch 5/5\n",
      "6354/6354 [==============================] - 959s 151ms/step - loss: 0.3463 - accuracy: 0.8819 - val_loss: 0.5236 - val_accuracy: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\EfficientNet\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\EfficientNet\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet model saved to: D:\\EfficientNet\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import os\n",
    "\n",
    "# Define a custom layer for casting to float32\n",
    "class CastToFloat32(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(inputs, tf.float32)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(CastToFloat32, self).get_config()\n",
    "        # No additional parameters to add\n",
    "        return config\n",
    "\n",
    "# Create an EfficientNetB0 model with ImageNet weights (excluding top)\n",
    "base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=IMAGE_SIZE + (3,))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Use a standard Dense layer (without custom dtype)\n",
    "dense_output = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "# Use our custom layer to cast to float32\n",
    "outputs = CastToFloat32()(dense_output)\n",
    "\n",
    "model_efficientnet = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "model_efficientnet.compile(optimizer=\"adam\",\n",
    "                           loss=\"sparse_categorical_crossentropy\",\n",
    "                           metrics=[\"accuracy\"])\n",
    "\n",
    "model_efficientnet.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output directory exists\n",
    "if not os.path.exists(model_output_directory):\n",
    "    os.makedirs(model_output_directory)\n",
    "\n",
    "efficientnet_save_path = r\"D:\\Virtual Environments\\Pattern Recognition\\Models\\EfficientNet\"\n",
    "\n",
    "# Save the model (SavedModel format)\n",
    "tf.saved_model.save(model_efficientnet, efficientnet_save_path)\n",
    "print(\"EfficientNet model saved to:\", efficientnet_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2734 files belonging to 10 classes.\n",
      "EfficientNet Evaluation Metrics:\n",
      "===================\n",
      "Accuracy:   0.8259\n",
      "Precision:  0.8462\n",
      "Recall:     0.8259\n",
      "F1 Score:   0.8295\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "test_data_path = r\"D:\\Virtual Environments\\Pattern Recognition\\segmented dataset\\test\"\n",
    "\n",
    "IMAGE_SIZE = (256, 256)  \n",
    "BATCH_SIZE = 8\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_path,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "model_save_path = r\"D:\\Virtual Environments\\Pattern Recognition\\Models\\EfficientNet\"\n",
    "loaded_model = tf.saved_model.load(model_save_path)\n",
    "infer = loaded_model.signatures[\"serving_default\"]\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    predictions = infer(images)\n",
    "    if isinstance(predictions, dict):\n",
    "        predictions = list(predictions.values())[0]\n",
    "    \n",
    "    preds_np = predictions.numpy()\n",
    "    batch_pred_labels = np.argmax(preds_np, axis=1)\n",
    "    \n",
    "    true_labels.extend(labels.numpy())\n",
    "    predicted_labels.extend(batch_pred_labels)\n",
    "\n",
    "# Compute metrics\n",
    "efficientnet_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "efficientnet_precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "efficientnet_recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "efficientnet_f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"EfficientNet Evaluation Metrics:\")\n",
    "print(\"===================\")\n",
    "print(f\"Accuracy:   {efficientnet_accuracy:.4f}\")\n",
    "print(f\"Precision:  {efficientnet_precision:.4f}\")\n",
    "print(f\"Recall:     {efficientnet_recall:.4f}\")\n",
    "print(f\"F1 Score:   {efficientnet_f1:.4f}\")\n",
    "print(\"===================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6354/6354 [==============================] - 1338s 209ms/step - loss: 1.3213 - accuracy: 0.5343 - val_loss: 6.8133 - val_accuracy: 0.1136\n",
      "Epoch 2/5\n",
      "6354/6354 [==============================] - 1339s 211ms/step - loss: 0.9607 - accuracy: 0.6653 - val_loss: 0.8171 - val_accuracy: 0.7300\n",
      "Epoch 3/5\n",
      "6354/6354 [==============================] - 1326s 209ms/step - loss: 0.7874 - accuracy: 0.7280 - val_loss: 2.2788 - val_accuracy: 0.4535\n",
      "Epoch 4/5\n",
      "6354/6354 [==============================] - 1321s 208ms/step - loss: 0.6637 - accuracy: 0.7695 - val_loss: 1.3995 - val_accuracy: 0.5999\n",
      "Epoch 5/5\n",
      "6354/6354 [==============================] - 1321s 208ms/step - loss: 0.5572 - accuracy: 0.8069 - val_loss: 1.5207 - val_accuracy: 0.5940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bb6dcdbf70>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import os\n",
    "\n",
    "IMAGE_SIZE = (256, 256)  \n",
    "NUM_CLASSES = 10         \n",
    "\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=IMAGE_SIZE + (3,))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model_resnet = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "model_resnet.compile(optimizer=\"adam\",\n",
    "                     loss=\"sparse_categorical_crossentropy\",\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "model_resnet.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Virtual Environments\\Pattern Recognition\\Models\\ResNet\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Virtual Environments\\Pattern Recognition\\Models\\ResNet\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:\\Virtual Environments\\Pattern Recognition\\Models\\ResNet\n"
     ]
    }
   ],
   "source": [
    "# Save the model using the SavedModel format to avoid JSON serialization issues.\n",
    "save_path = r\"D:\\Virtual Environments\\Pattern Recognition\\Models\\ResNet\"\n",
    "tf.saved_model.save(model_resnet, save_path)\n",
    "print(\"Model saved to:\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2734 files belonging to 10 classes.\n",
      "ResNet Evaluation Metrics:\n",
      "===================\n",
      "Accuracy:   0.5801\n",
      "Precision:  0.7253\n",
      "Recall:     0.5801\n",
      "F1 Score:   0.5818\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "test_data_path = r\"D:\\Virtual Environments\\Pattern Recognition\\segmented dataset\\test\"\n",
    "\n",
    "IMAGE_SIZE = (256, 256)  \n",
    "BATCH_SIZE = 8\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_path,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "model_save_path = r\"D:\\Virtual Environments\\Pattern Recognition\\Models\\ResNet\"\n",
    "loaded_model = tf.saved_model.load(model_save_path)\n",
    "infer = loaded_model.signatures[\"serving_default\"]\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    predictions = infer(images)\n",
    "    if isinstance(predictions, dict):\n",
    "        predictions = list(predictions.values())[0]\n",
    "    \n",
    "    preds_np = predictions.numpy()\n",
    "    batch_pred_labels = np.argmax(preds_np, axis=1)\n",
    "    \n",
    "    true_labels.extend(labels.numpy())\n",
    "    predicted_labels.extend(batch_pred_labels)\n",
    "\n",
    "# Compute metrics\n",
    "resnet_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "resnet_precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "resnet_recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "resnet_f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"ResNet Evaluation Metrics:\")\n",
    "print(\"===================\")\n",
    "print(f\"Accuracy:   {resnet_accuracy:.4f}\")\n",
    "print(f\"Precision:  {resnet_precision:.4f}\")\n",
    "print(f\"Recall:     {resnet_recall:.4f}\")\n",
    "print(f\"F1 Score:   {resnet_f1:.4f}\")\n",
    "print(\"===================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6354/6354 [==============================] - 1887s 296ms/step - loss: 1.7444 - accuracy: 0.4012 - val_loss: 1.2528 - val_accuracy: 0.5462\n",
      "Epoch 2/5\n",
      "6354/6354 [==============================] - 1864s 293ms/step - loss: 1.2921 - accuracy: 0.5387 - val_loss: 1.2279 - val_accuracy: 0.5642\n",
      "Epoch 3/5\n",
      "6354/6354 [==============================] - 1861s 293ms/step - loss: 1.2100 - accuracy: 0.5749 - val_loss: 1.2157 - val_accuracy: 0.5987\n",
      "Epoch 4/5\n",
      "6354/6354 [==============================] - 1861s 293ms/step - loss: 1.1809 - accuracy: 0.5872 - val_loss: 1.2894 - val_accuracy: 0.5594\n",
      "Epoch 5/5\n",
      "6354/6354 [==============================] - 1861s 293ms/step - loss: 1.1649 - accuracy: 0.5904 - val_loss: 1.0581 - val_accuracy: 0.6388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2baa6332b20>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import os\n",
    "\n",
    "IMAGE_SIZE = (256, 256)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Create a VGG16 model with ImageNet weights, excluding top layers\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=IMAGE_SIZE + (3,))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model_vgg = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "model_vgg.compile(optimizer=\"adam\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "# Ensure you have train_ds and val_ds defined as your training and validation datasets.\n",
    "model_vgg.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Virtual Environments\\Pattern Recognition\\Models\\VGG\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Virtual Environments\\Pattern Recognition\\Models\\VGG\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: D:\\Virtual Environments\\Pattern Recognition\\Models\\VGG\n"
     ]
    }
   ],
   "source": [
    "# Save the model using the SavedModel format\n",
    "save_path = r\"D:\\Virtual Environments\\Pattern Recognition\\Models\\VGG\"\n",
    "tf.saved_model.save(model_vgg, save_path)\n",
    "print(\"Model saved to:\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2734 files belonging to 10 classes.\n",
      "VGG Evaluation Metrics:\n",
      "===================\n",
      "Accuracy:   0.5999\n",
      "Precision:  0.6223\n",
      "Recall:     0.5999\n",
      "F1 Score:   0.5998\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "test_data_path = r\"D:\\Virtual Environments\\Pattern Recognition\\segmented dataset\\test\"\n",
    "\n",
    "IMAGE_SIZE = (256, 256)  \n",
    "BATCH_SIZE = 8\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_path,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE\n",
    ")\n",
    "\n",
    "model_save_path = r\"D:\\Virtual Environments\\Pattern Recognition\\Models\\VGG\"\n",
    "loaded_model = tf.saved_model.load(model_save_path)\n",
    "infer = loaded_model.signatures[\"serving_default\"]\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    predictions = infer(images)\n",
    "    if isinstance(predictions, dict):\n",
    "        predictions = list(predictions.values())[0]\n",
    "    \n",
    "    preds_np = predictions.numpy()\n",
    "    batch_pred_labels = np.argmax(preds_np, axis=1)\n",
    "    \n",
    "    true_labels.extend(labels.numpy())\n",
    "    predicted_labels.extend(batch_pred_labels)\n",
    "\n",
    "vgg_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "vgg_precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "vgg_recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "vgg_f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"VGG Evaluation Metrics:\")\n",
    "print(\"===================\")\n",
    "print(f\"Accuracy:   {vgg_accuracy:.4f}\")\n",
    "print(f\"Precision:  {vgg_precision:.4f}\")\n",
    "print(f\"Recall:     {vgg_recall:.4f}\")\n",
    "print(f\"F1 Score:   {vgg_f1:.4f}\")\n",
    "print(\"===================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['EfficientNet', 'ResNet', 'VGG', \"Stacking Architecture\"]\n",
    "values = [efficientnet_f1, resnet_f1, vgg_f1 , 0.87]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(metrics, values, color=['cyan', 'yellow', 'red' , 'purple'])\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Comparison')\n",
    "\n",
    "# Annotate the bars with their values\n",
    "for bar, val in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.01, f'{val:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

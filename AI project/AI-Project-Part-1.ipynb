{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Splitting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "op_dataset_raw = load_dataset(\"csv\" , data_files=r\"D:\\Virtual Environments\\Thesis\\datasets\\OpSpam dataset.csv\")\n",
    "op_dataset_raw = op_dataset_raw[\"train\"].remove_columns(['hotel' , 'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing op dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['truthful_positive', 'truthful_negative', 'deceptive_positive', 'deceptive_negative']\n"
     ]
    }
   ],
   "source": [
    "def combine_labels(record):\n",
    "    record[\"label\"] = record[\"deceptive\"] + \"_\" + record[\"polarity\"]\n",
    "    return record\n",
    "op_dataset_raw = op_dataset_raw.map(combine_labels)\n",
    "op_dataset_raw = op_dataset_raw.remove_columns([\"deceptive\" , \"polarity\"])\n",
    "\n",
    "unique_classes = list(set(op_dataset_raw[\"label\"]))\n",
    "unique_classes.sort(reverse=True)\n",
    "print(unique_classes)\n",
    "\n",
    "def one_hot_encoding(record):\n",
    "    record[\"label\"] = unique_classes.index(record[\"label\"])\n",
    "    return record\n",
    "\n",
    "op_dataset_raw = op_dataset_raw.map(one_hot_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'We stayed for a one night getaway with family on a thursday. Triple AAA rate of 173 was a steal. 7th floor room complete with 44in plasma TV bose stereo, voss and evian water, and gorgeous bathroom(no tub but was fine for us) Concierge was very helpful. You cannot beat this location... Only flaw was breakfast was pricey and service was very very slow(2hours for four kids and four adults on a friday morning) even though there were only two other tables in the restaurant. Food was very good so it was worth the wait. I would return in a heartbeat. A gem in chicago... \\n',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_dataset_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df1 = op_dataset_raw.to_pandas()\n",
    "\n",
    "# Use pandas value_counts to count the labels\n",
    "label_counts = df1['label'].value_counts().sort_index()\n",
    "\n",
    "label_counts.plot(kind='bar', figsize=(8, 5), color='cyan')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Number of Records')\n",
    "plt.title('Number of Records per Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "split_dataset_op = op_dataset_raw.train_test_split(test_size=0.1 , seed=42)\n",
    "train_test_split_op = split_dataset_op[\"train\"].train_test_split(test_size=0.1111 , seed=42)\n",
    "op_dataset = DatasetDict({\n",
    "    \"train\" : train_test_split_op[\"train\"],\n",
    "    \"test\" : train_test_split_op[\"test\"],\n",
    "    \"validation\" : split_dataset_op[\"test\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1280\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# tokenizer also passed to ensure used tokenizer is of same checkpoint as the model\n",
    "# Dynamic Padding. Hence, dp\n",
    "def tokenize(example , tokenizer):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_checkpoint = 'google/electra-base-discriminator'\n",
    "electra_tokenizer = AutoTokenizer.from_pretrained(electra_checkpoint)\n",
    "tokenized_dataset_dp_electra_op = op_dataset.map(tokenize, batched=True, fn_kwargs={'tokenizer': electra_tokenizer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_checkpoint = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
    "tokenized_dataset_dp_bert_op = op_dataset.map(tokenize , batched = True , fn_kwargs = {'tokenizer' : bert_tokenizer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_checkpoint)\n",
    "tokenized_dataset_dp_distilbert_op = op_dataset.map(tokenize, batched=True , fn_kwargs= {'tokenizer' : distilbert_tokenizer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_checkpoint = \"FacebookAI/roberta-base\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint)\n",
    "tokenized_dataset_dp_roberta_op = op_dataset.map(tokenize, batched=True , fn_kwargs= {'tokenizer' : roberta_tokenizer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(evaluation_predictions):\n",
    "  logits , actual_labels = evaluation_predictions\n",
    "  accuracy_metric = evaluate.load(\"accuracy\")\n",
    "  f1_metric = evaluate.load(\"f1\")\n",
    "  precision_metric = evaluate.load(\"precision\")\n",
    "  recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "  predicted_labels = np.argmax(logits , axis = -1)\n",
    "  accuracy = accuracy_metric.compute(predictions=predicted_labels , references=actual_labels)\n",
    "  f1 = f1_metric.compute(predictions=predicted_labels , references=actual_labels, average='macro')\n",
    "  precision = precision_metric.compute(predictions=predicted_labels , references=actual_labels , average='macro')\n",
    "  recall = recall_metric.compute(predictions=predicted_labels , references=actual_labels, average='macro')\n",
    "\n",
    "  return {**accuracy , **f1 , **precision , **recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 3:01:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686313</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.776525</td>\n",
       "      <td>0.810812</td>\n",
       "      <td>0.778551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.455159</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.812820</td>\n",
       "      <td>0.846578</td>\n",
       "      <td>0.811080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313603</td>\n",
       "      <td>0.893750</td>\n",
       "      <td>0.892086</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.894744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457251</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.821222</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.817187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.355101</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.867921</td>\n",
       "      <td>0.875199</td>\n",
       "      <td>0.865341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer , Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(electra_checkpoint , num_labels=4)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=electra_tokenizer)\n",
    "\n",
    "output_directory = './models/Electra'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_directory,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate = 0.00002,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    report_to = []\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_dataset_dp_electra_op['train'],\n",
    "    eval_dataset = tokenized_dataset_dp_electra_op['validation'],\n",
    "    data_collator = data_collator,\n",
    "    processing_class = electra_tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 2:41:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.508655</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.825383</td>\n",
       "      <td>0.829786</td>\n",
       "      <td>0.824290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321941</td>\n",
       "      <td>0.881250</td>\n",
       "      <td>0.878673</td>\n",
       "      <td>0.880753</td>\n",
       "      <td>0.883807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.302181</td>\n",
       "      <td>0.881250</td>\n",
       "      <td>0.876130</td>\n",
       "      <td>0.876809</td>\n",
       "      <td>0.875852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.332754</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.881540</td>\n",
       "      <td>0.885020</td>\n",
       "      <td>0.880966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.328577</td>\n",
       "      <td>0.881250</td>\n",
       "      <td>0.875523</td>\n",
       "      <td>0.877727</td>\n",
       "      <td>0.874290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer , Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(bert_checkpoint , num_labels=4)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)\n",
    "\n",
    "output_directory = './models/BERT'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_directory,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate = 0.00002,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    report_to = []\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_dataset_dp_bert_op['train'],\n",
    "    eval_dataset = tokenized_dataset_dp_bert_op['validation'],\n",
    "    data_collator = data_collator,\n",
    "    processing_class = bert_tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 31:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.575929</td>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.846187</td>\n",
       "      <td>0.852819</td>\n",
       "      <td>0.844034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.355498</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.883422</td>\n",
       "      <td>0.884182</td>\n",
       "      <td>0.885227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.300006</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.897029</td>\n",
       "      <td>0.896316</td>\n",
       "      <td>0.899858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.283626</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.893974</td>\n",
       "      <td>0.902034</td>\n",
       "      <td>0.890767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270121</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.900761</td>\n",
       "      <td>0.903667</td>\n",
       "      <td>0.899148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer , Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(distilbert_checkpoint, num_labels=4)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=distilbert_tokenizer)\n",
    "\n",
    "output_directory = './models/DistilBERT'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_directory,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate = 0.00002,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    report_to = []\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_dataset_dp_distilbert_op['train'],\n",
    "    eval_dataset = tokenized_dataset_dp_distilbert_op['validation'],\n",
    "    data_collator = data_collator,\n",
    "    processing_class = distilbert_tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\transformers\\training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 1:59:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.310002</td>\n",
       "      <td>0.893750</td>\n",
       "      <td>0.887148</td>\n",
       "      <td>0.892503</td>\n",
       "      <td>0.886364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.274751</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.897684</td>\n",
       "      <td>0.904654</td>\n",
       "      <td>0.906960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225616</td>\n",
       "      <td>0.943750</td>\n",
       "      <td>0.941194</td>\n",
       "      <td>0.941540</td>\n",
       "      <td>0.941335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.284670</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.933540</td>\n",
       "      <td>0.944171</td>\n",
       "      <td>0.928693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.256241</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.934250</td>\n",
       "      <td>0.935417</td>\n",
       "      <td>0.933523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer , Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint , num_labels=4)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "\n",
    "output_directory = './models/RoBERTa'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_directory,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate = 0.00002,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    report_to = [],\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_dataset_dp_roberta_op['train'],\n",
    "    eval_dataset = tokenized_dataset_dp_roberta_op['validation'],\n",
    "    data_collator = data_collator,\n",
    "    processing_class = roberta_tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Learner Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "electra = AutoModelForSequenceClassification.from_pretrained('./models/Electra')\n",
    "electra_tokenizer = AutoTokenizer.from_pretrained('./models/Electra')\n",
    "\n",
    "bert = AutoModelForSequenceClassification.from_pretrained('./models/BERT')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('./models/Electra')\n",
    "\n",
    "distilbert = AutoModelForSequenceClassification.from_pretrained('./models/DistilBERT')\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained('./models/Electra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './predictions',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    ")\n",
    "\n",
    "electra_trainer = Trainer(\n",
    "    electra,\n",
    "    training_args,\n",
    "    processing_class = electra_tokenizer\n",
    ")\n",
    "\n",
    "bert_trainer = Trainer(\n",
    "    bert,\n",
    "    training_args,\n",
    "    processing_class = bert_tokenizer\n",
    ")\n",
    "\n",
    "distilbert_trainer = Trainer(\n",
    "    distilbert,\n",
    "    training_args,\n",
    "    processing_class = distilbert_tokenizer\n",
    ")\n",
    "\n",
    "tokenized_dataset_dp_electra_op_combined = concatenate_datasets([tokenized_dataset_dp_electra_op['test'] , tokenized_dataset_dp_electra_op['validation']])\n",
    "tokenized_dataset_dp_bert_op_combined = concatenate_datasets([tokenized_dataset_dp_bert_op['test'] , tokenized_dataset_dp_bert_op['validation']])\n",
    "tokenized_dataset_dp_distilbert_op_combined = concatenate_datasets([tokenized_dataset_dp_distilbert_op['test'] , tokenized_dataset_dp_distilbert_op['validation']])\n",
    "\n",
    "\n",
    "electra_outputs = electra_trainer.predict(tokenized_dataset_dp_electra_op_combined)\n",
    "bert_outputs = bert_trainer.predict(tokenized_dataset_dp_bert_op_combined)\n",
    "distilbert_outputs = distilbert_trainer.predict(tokenized_dataset_dp_distilbert_op_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.66308486, -1.43721   ,  3.0861626 , -0.8970183 ],\n",
       "       [-0.7137503 , -1.4687352 ,  3.080502  , -0.89906025],\n",
       "       [ 3.1207328 , -0.94269246, -0.48463607, -1.6172389 ],\n",
       "       [ 3.106535  , -0.8019607 , -0.6701002 , -1.5754206 ],\n",
       "       [-0.01979489,  2.5679324 , -1.5783356 , -0.8314981 ],\n",
       "       [-0.36806595, -1.531097  ,  3.0728288 , -1.1968529 ],\n",
       "       [ 3.0462546 , -1.017493  , -0.3505312 , -1.6441224 ],\n",
       "       [ 3.0673099 , -0.9944266 , -0.39594027, -1.5871654 ],\n",
       "       [ 3.0775893 , -0.661633  , -0.8383566 , -1.3635106 ],\n",
       "       [-0.56425625, -1.4854238 ,  3.0989633 , -1.0565637 ],\n",
       "       [-1.2565767 ,  2.9103448 , -1.7768347 ,  0.12567131],\n",
       "       [-0.736255  , -1.517362  ,  3.0635262 , -0.7803748 ],\n",
       "       [-1.3402449 ,  2.6991227 , -1.7931138 ,  0.48812518],\n",
       "       [-1.6206127 , -0.37629747, -0.9533299 ,  2.9962473 ],\n",
       "       [-0.99873054,  2.893582  , -1.5580086 , -0.30675617],\n",
       "       [-0.74591523, -1.4871135 ,  3.10499   , -0.9079613 ],\n",
       "       [-0.61494565, -1.4974213 ,  3.131253  , -0.9917117 ],\n",
       "       [ 0.6582202 , -1.7168607 ,  2.719654  , -1.7628732 ],\n",
       "       [-1.6506944 , -0.5184017 , -0.7561615 ,  2.958601  ],\n",
       "       [-0.02030522, -1.6064365 ,  3.045327  , -1.4672086 ],\n",
       "       [ 3.1317136 , -1.0726742 , -0.28158876, -1.7181396 ],\n",
       "       [ 3.127609  , -0.74003637, -0.7791735 , -1.4537553 ],\n",
       "       [-1.5305473 , -0.28970474, -1.0792159 ,  2.9676268 ],\n",
       "       [-1.6312017 , -0.48662597, -0.946289  ,  3.0604358 ],\n",
       "       [-1.1094013 ,  2.8831778 , -1.4882277 , -0.2505998 ],\n",
       "       [-1.5821104 ,  2.6736262 , -1.7765614 ,  0.6392065 ],\n",
       "       [-1.761256  ,  0.7288821 , -1.6907524 ,  2.7808647 ],\n",
       "       [-1.5399758 , -0.50013256, -0.8351445 ,  2.9655106 ],\n",
       "       [-1.5947359 , -0.64733666, -0.59043515,  2.8849475 ],\n",
       "       [-0.970228  ,  2.9301708 , -1.5682381 , -0.26894805],\n",
       "       [-1.3105539 ,  2.9154134 , -1.8322142 ,  0.24142437],\n",
       "       [-0.34506965,  2.7590833 , -1.5980579 , -0.66067547],\n",
       "       [ 1.8327465 , -1.6315123 ,  1.6933315 , -1.9711816 ],\n",
       "       [-1.5122447 , -0.5390127 , -0.5407932 ,  2.78335   ],\n",
       "       [-0.71472245, -1.4823849 ,  3.111293  , -0.8718277 ],\n",
       "       [-1.8486792 ,  0.90119773, -1.6847177 ,  2.5754604 ],\n",
       "       [-0.6858877 , -1.468163  ,  3.0947926 , -0.9559349 ],\n",
       "       [-1.8415158 ,  0.33230454, -1.5327096 ,  2.9904294 ],\n",
       "       [ 3.0317123 , -0.55684966, -0.86007464, -1.4376311 ],\n",
       "       [ 3.0989532 , -1.0022484 , -0.365951  , -1.6844102 ],\n",
       "       [-1.1061249 ,  2.8967903 , -1.6263458 , -0.12567867],\n",
       "       [-0.7465766 , -1.4673482 ,  3.0656052 , -0.79522216],\n",
       "       [-1.557682  , -0.57412153, -0.7820861 ,  2.9765837 ],\n",
       "       [ 2.9404964 , -1.2048854 ,  0.05868999, -1.8101041 ],\n",
       "       [-0.19681281, -1.5404764 ,  3.0780718 , -1.3250962 ],\n",
       "       [-1.5414236 , -0.9468428 ,  0.67402726,  1.8756162 ],\n",
       "       [-0.7752435 , -1.4623635 ,  3.0900645 , -0.80421245],\n",
       "       [-1.1779335 ,  2.8932922 , -1.6593245 , -0.02305434],\n",
       "       [-0.7183733 , -1.4518098 ,  3.0587838 , -0.8801184 ],\n",
       "       [-1.677352  ,  0.14759682, -1.3718641 ,  3.0035853 ],\n",
       "       [ 3.105773  , -0.8888265 , -0.49772823, -1.6211715 ],\n",
       "       [ 3.108436  , -0.892832  , -0.49957258, -1.670018  ],\n",
       "       [-1.5626149 , -0.64781433, -0.5980208 ,  2.854304  ],\n",
       "       [-1.5572141 , -0.5779451 , -0.74623966,  2.9794948 ],\n",
       "       [-0.7628641 , -1.4751272 ,  3.0803962 , -0.8673445 ],\n",
       "       [-0.60011023, -1.5128927 ,  3.0653858 , -0.98447776],\n",
       "       [-1.3018888 ,  2.913157  , -1.6563244 ,  0.02548806],\n",
       "       [-1.7845092 , -0.03596458, -1.288924  ,  3.066206  ],\n",
       "       [-1.7860272 ,  0.15980048, -1.2945813 ,  2.8921227 ],\n",
       "       [-1.5797781 , -0.4992771 , -0.7747405 ,  2.9170415 ],\n",
       "       [ 3.1314266 , -0.8318406 , -0.6213242 , -1.5765071 ],\n",
       "       [-1.3512934 ,  2.9146762 , -1.7555547 ,  0.18183522],\n",
       "       [-1.622482  , -0.48976636, -0.8100876 ,  3.0039787 ],\n",
       "       [-0.7792224 ,  2.858025  , -1.473405  , -0.513801  ],\n",
       "       [-0.78592587, -1.4463733 ,  3.0246117 , -0.77725494],\n",
       "       [-1.6464734 , -0.63672256, -0.6007596 ,  2.9165196 ],\n",
       "       [-0.8750959 ,  2.9336586 , -1.602462  , -0.3941936 ],\n",
       "       [-1.564679  , -0.5697174 , -0.7921829 ,  2.9458528 ],\n",
       "       [-0.7488523 , -1.4745793 ,  3.1234589 , -0.8879869 ],\n",
       "       [-0.26435035,  2.6239998 , -1.5009756 , -0.6861383 ],\n",
       "       [-1.8319402 ,  0.96958613, -1.8077551 ,  2.6095226 ],\n",
       "       [-1.5401042 , -0.5049017 , -0.87103117,  3.0026977 ],\n",
       "       [-1.8439713 ,  0.43379533, -1.5489011 ,  2.92665   ],\n",
       "       [ 3.0861552 , -0.80494994, -0.64883214, -1.467619  ],\n",
       "       [ 2.9825747 , -1.1400515 , -0.07208203, -1.7449541 ],\n",
       "       [ 3.0216444 , -0.53391004, -0.9357946 , -1.380192  ],\n",
       "       [-0.29104224, -1.5237929 ,  3.124285  , -1.2510773 ],\n",
       "       [ 2.919812  , -0.93951344, -0.398578  , -1.5260428 ],\n",
       "       [ 3.143828  , -0.855824  , -0.62254566, -1.5424502 ],\n",
       "       [-0.5487555 ,  2.8110251 , -1.4875153 , -0.62269664],\n",
       "       [-0.73484635, -1.4908167 ,  3.0680552 , -0.9193924 ],\n",
       "       [-0.5540992 ,  2.7974355 , -1.7823935 , -0.41393647],\n",
       "       [-0.8806623 ,  2.8300054 , -1.4282701 , -0.45896137],\n",
       "       [-1.6965659 , -0.30718318, -1.0772945 ,  3.0915322 ],\n",
       "       [-0.8452869 , -1.465099  ,  3.038038  , -0.73127323],\n",
       "       [-1.6839594 , -0.2578221 , -1.1209831 ,  3.077415  ],\n",
       "       [ 0.8488635 , -1.6420945 ,  2.5082462 , -1.842471  ],\n",
       "       [ 3.0201488 , -0.600891  , -0.80791336, -1.4465853 ],\n",
       "       [-0.18491426,  2.6515625 , -1.5798396 , -0.7021252 ],\n",
       "       [-0.69917977, -1.4710557 ,  3.123713  , -0.9497814 ],\n",
       "       [-1.7663585 ,  2.0744581 , -2.053546  ,  1.7428752 ],\n",
       "       [-0.63940036, -1.5002395 ,  3.1086023 , -0.93361044],\n",
       "       [ 3.1015902 , -0.8422713 , -0.57875973, -1.5652676 ],\n",
       "       [-1.7982253 ,  2.0096145 , -1.9252166 ,  1.6165953 ],\n",
       "       [-1.6464605 , -0.5794336 , -0.62517357,  2.9280367 ],\n",
       "       [-1.6994835 , -0.26676577, -1.1184157 ,  3.094099  ],\n",
       "       [ 0.26176345, -1.6648003 ,  2.9438558 , -1.6302546 ],\n",
       "       [-0.8313133 , -1.397788  ,  2.9764438 , -0.6608137 ],\n",
       "       [ 3.1115098 , -0.8395498 , -0.61989915, -1.5153652 ],\n",
       "       [-0.88781977, -1.443358  ,  2.9697545 , -0.66512674],\n",
       "       [ 1.2694229 ,  1.5187097 , -1.640847  , -0.9399618 ],\n",
       "       [-0.44065207,  2.7722876 , -1.7423588 , -0.45011926],\n",
       "       [-1.733759  ,  1.0174505 , -1.8723252 ,  2.5943239 ],\n",
       "       [-0.81927425, -1.4156392 ,  3.031506  , -0.7444524 ],\n",
       "       [-0.974999  ,  2.8912427 , -1.5162874 , -0.3538836 ],\n",
       "       [-0.72435606, -1.5040203 ,  3.0647824 , -0.83214486],\n",
       "       [-0.74503785, -1.4507563 ,  3.0169683 , -0.79027575],\n",
       "       [ 1.3296212 , -1.7560966 ,  2.2087765 , -1.974174  ],\n",
       "       [-0.57982886, -1.4813753 ,  3.1454244 , -1.0723742 ],\n",
       "       [-1.6485779 , -0.04746833, -1.1642184 ,  2.9465857 ],\n",
       "       [-1.5621169 ,  2.553143  , -1.9934282 ,  1.0424405 ],\n",
       "       [-0.85739607, -1.4459735 ,  3.007741  , -0.6841866 ],\n",
       "       [-1.5748363 , -0.4701982 , -0.8721869 ,  3.0293417 ],\n",
       "       [-0.8975969 ,  2.8491561 , -1.3878849 , -0.49196327],\n",
       "       [ 2.13348   , -1.601876  ,  1.4198543 , -2.004967  ],\n",
       "       [ 3.123356  , -0.77424604, -0.66398335, -1.4671541 ],\n",
       "       [-1.5153167 , -0.6443928 , -0.56240284,  2.8313744 ],\n",
       "       [-0.5487555 ,  2.8110251 , -1.4875153 , -0.62269664],\n",
       "       [-1.5283073 ,  2.6691165 , -1.9115806 ,  0.7226765 ],\n",
       "       [-1.6844556 , -0.29970947, -1.0989676 ,  3.091437  ],\n",
       "       [-0.77612656, -1.4695883 ,  3.0821834 , -0.8377245 ],\n",
       "       [-0.8166471 , -1.2952409 ,  2.5059752 , -0.37435776],\n",
       "       [ 3.147622  , -0.8838458 , -0.59888375, -1.5477209 ],\n",
       "       [-1.6067233 , -0.4945107 , -0.8924997 ,  3.053425  ],\n",
       "       [-1.605737  , -0.55232805, -0.7712858 ,  2.9565544 ],\n",
       "       [ 3.0365982 , -1.1695582 , -0.11476099, -1.7968799 ],\n",
       "       [-1.5616556 ,  2.646408  , -1.929015  ,  0.8080006 ],\n",
       "       [ 3.0408201 , -1.042942  , -0.29205233, -1.6589401 ],\n",
       "       [ 0.8867372 , -1.7413026 ,  2.6020093 , -1.9019042 ],\n",
       "       [-1.6296006 , -0.49196076, -0.8899075 ,  3.043964  ],\n",
       "       [-0.6985618 , -1.489525  ,  3.1205282 , -0.886296  ],\n",
       "       [-1.6050571 , -0.60522234, -0.54466337,  2.8190887 ],\n",
       "       [ 1.5437574 , -1.6842413 ,  1.9891324 , -1.9657001 ],\n",
       "       [-1.5022014 ,  2.7140455 , -1.806076  ,  0.5167547 ],\n",
       "       [-1.5762379 ,  2.273277  , -2.0728128 ,  1.3952501 ],\n",
       "       [ 2.7488925 , -1.3627405 ,  0.4743909 , -1.9153987 ],\n",
       "       [-1.5565724 , -0.46926457, -0.93354374,  2.9985464 ],\n",
       "       [-1.7327741 , -0.42795762, -0.931929  ,  3.0206618 ],\n",
       "       [ 3.0961728 , -0.7122564 , -0.7715243 , -1.4307984 ],\n",
       "       [ 3.0730717 , -1.0268172 , -0.2536459 , -1.7640078 ],\n",
       "       [-0.27521318, -1.5738766 ,  3.1339014 , -1.2741702 ],\n",
       "       [ 1.8795136 , -1.676826  ,  1.7154092 , -2.0563827 ],\n",
       "       [-0.57765055, -1.4722477 ,  3.1358387 , -1.0039047 ],\n",
       "       [-1.8632206 ,  1.2194166 , -1.8445892 ,  2.3814712 ],\n",
       "       [-1.2741811 ,  2.913705  , -1.6948329 ,  0.10936556],\n",
       "       [-1.4609952 , -0.76986796,  0.20704152,  1.9500203 ],\n",
       "       [ 2.9344866 , -0.95512664, -0.28603008, -1.6262689 ],\n",
       "       [-1.6115183 , -0.38591886, -0.9976635 ,  3.0633528 ],\n",
       "       [-1.6373501 , -0.62098235, -0.487783  ,  2.8172987 ],\n",
       "       [-0.85531354, -1.4677742 ,  3.002081  , -0.6451863 ],\n",
       "       [-1.2529784 ,  2.9236135 , -1.6035242 , -0.07465941],\n",
       "       [-1.6209977 , -0.5110872 , -0.7886088 ,  2.9761646 ],\n",
       "       [-1.5647227 ,  2.5796368 , -1.8391345 ,  0.8403168 ],\n",
       "       [-1.7643353 , -0.13856636, -1.221749  ,  3.1085396 ],\n",
       "       [-1.5195568 ,  2.8127832 , -1.8495814 ,  0.5451168 ],\n",
       "       [-0.66552246,  2.890177  , -1.5885669 , -0.51014173],\n",
       "       [-1.9257071 ,  0.9622654 , -1.6527933 ,  2.5974314 ],\n",
       "       [ 3.1202426 , -0.84481454, -0.6560406 , -1.5075054 ],\n",
       "       [-1.5879861 , -0.49449468, -0.80752087,  2.9835155 ],\n",
       "       [-1.8044419 ,  1.3930626 , -1.9544992 ,  2.3286576 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_outputs.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Combined Dataframe For Input to Meta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>electra_0</th>\n",
       "      <th>electra_1</th>\n",
       "      <th>electra_2</th>\n",
       "      <th>electra_3</th>\n",
       "      <th>bert_0</th>\n",
       "      <th>bert_1</th>\n",
       "      <th>bert_2</th>\n",
       "      <th>bert_3</th>\n",
       "      <th>distilbert_0</th>\n",
       "      <th>distilbert_1</th>\n",
       "      <th>distilbert_2</th>\n",
       "      <th>distilbert_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022351</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>0.949655</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.979695</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>0.964071</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021392</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>0.950779</td>\n",
       "      <td>0.017774</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>0.984753</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>0.018267</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.966914</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.949556</td>\n",
       "      <td>0.016323</td>\n",
       "      <td>0.025806</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>0.983233</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.967923</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.950361</td>\n",
       "      <td>0.019074</td>\n",
       "      <td>0.021763</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.985473</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.964231</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.066871</td>\n",
       "      <td>0.889358</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.029698</td>\n",
       "      <td>0.020030</td>\n",
       "      <td>0.958933</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.461232</td>\n",
       "      <td>0.515698</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   electra_0  electra_1  electra_2  electra_3    bert_0    bert_1    bert_2  \\\n",
       "0   0.022351   0.010306   0.949655   0.017689  0.007171  0.002790  0.979695   \n",
       "1   0.021392   0.010055   0.950779   0.017774  0.006442  0.002603  0.984753   \n",
       "2   0.949556   0.016323   0.025806   0.008315  0.983233  0.004432  0.010557   \n",
       "3   0.950361   0.019074   0.021763   0.008801  0.985473  0.004766  0.007571   \n",
       "4   0.066871   0.889358   0.014073   0.029698  0.020030  0.958933  0.005812   \n",
       "\n",
       "     bert_3  distilbert_0  distilbert_1  distilbert_2  distilbert_3  label  \n",
       "0  0.010344      0.017188      0.006783      0.964071      0.011957      2  \n",
       "1  0.006202      0.018267      0.005162      0.966914      0.009657      2  \n",
       "2  0.001778      0.967923      0.012887      0.015805      0.003385      0  \n",
       "3  0.002190      0.964231      0.010292      0.021584      0.003893      0  \n",
       "4  0.015225      0.461232      0.515698      0.010906      0.012164      1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "electra_predictions = torch.nn.functional.softmax(tensor(electra_outputs.predictions), dim=-1)\n",
    "bert_predictions = torch.nn.functional.softmax(tensor(bert_outputs.predictions), dim=-1)\n",
    "distilbert_predictions = torch.nn.functional.softmax(tensor(distilbert_outputs.predictions), dim=-1)\n",
    "\n",
    "electra_predictions_0 = [item[0].item() for item in electra_predictions]\n",
    "electra_predictions_1 = [item[1].item() for item in electra_predictions]\n",
    "electra_predictions_2 = [item[2].item() for item in electra_predictions]\n",
    "electra_predictions_3 = [item[3].item() for item in electra_predictions]\n",
    "\n",
    "bert_predictions_0 = [item[0].item() for item in bert_predictions]\n",
    "bert_predictions_1 = [item[1].item() for item in bert_predictions]\n",
    "bert_predictions_2 = [item[2].item() for item in bert_predictions]\n",
    "bert_predictions_3 = [item[3].item() for item in bert_predictions]\n",
    "\n",
    "distilbert_predictions_0 = [item[0].item() for item in distilbert_predictions]\n",
    "distilbert_predictions_1 = [item[1].item() for item in distilbert_predictions]\n",
    "distilbert_predictions_2 = [item[2].item() for item in distilbert_predictions]\n",
    "distilbert_predictions_3 = [item[3].item() for item in distilbert_predictions]\n",
    "\n",
    "\n",
    "\n",
    "combined_dataframe = pd.DataFrame({\n",
    "    'electra_0': electra_predictions_0,\n",
    "    'electra_1' : electra_predictions_1,\n",
    "    'electra_2' : electra_predictions_2,\n",
    "    'electra_3' : electra_predictions_3,\n",
    "    'bert_0' : bert_predictions_0,\n",
    "    'bert_1' : bert_predictions_1,\n",
    "    'bert_2' : bert_predictions_2,\n",
    "    'bert_3' : bert_predictions_3,\n",
    "    'distilbert_0' : distilbert_predictions_0,\n",
    "    'distilbert_1' : distilbert_predictions_1,\n",
    "    'distilbert_2' : distilbert_predictions_2,\n",
    "    'distilbert_3' : distilbert_predictions_3,\n",
    "    'label' : op_dataset['test']['label'] + op_dataset['validation']['label']\n",
    "})\n",
    "combined_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self , input_dim , num_labels):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim , 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None , labels=None , **kwargs):\n",
    "        logits = self.classifier(input_ids.float())\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels.long())\n",
    "\n",
    "        return {\"loss\" : loss , \"logits\" : logits}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_hf_dataset(df, feature_cols, label_col):\n",
    "    ds = Dataset.from_dict({\n",
    "        col: df[col].tolist() for col in feature_cols + [label_col]\n",
    "    })\n",
    "    ds = ds.rename_column(label_col, \"labels\")  # HF expects a 'labels' column\n",
    "    return ds\n",
    "\n",
    "feature_cols = ['electra_0', 'electra_1', 'electra_2', 'electra_3', 'bert_0', 'bert_1', 'bert_2', 'bert_3', 'distilbert_0', 'distilbert_1', 'distilbert_2','distilbert_3']\n",
    "label_col = 'label'\n",
    "combined_dataset = df_to_hf_dataset(combined_dataframe, feature_cols, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['electra_0', 'electra_1', 'electra_2', 'electra_3', 'bert_0', 'bert_1', 'bert_2', 'bert_3', 'distilbert_0', 'distilbert_1', 'distilbert_2', 'distilbert_3', 'labels'],\n",
       "        num_rows: 128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['electra_0', 'electra_1', 'electra_2', 'electra_3', 'bert_0', 'bert_1', 'bert_2', 'bert_3', 'distilbert_0', 'distilbert_1', 'distilbert_2', 'distilbert_3', 'labels'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset = combined_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Convert each example into PyTorch tensors\n",
    "    input_ids = torch.tensor([ [ex[col] for col in feature_cols] for ex in batch ])\n",
    "    labels = torch.tensor([ex[\"labels\"] for ex in batch])\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'electra_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 26\u001b[0m\n\u001b[0;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      7\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m output_dir,\n\u001b[0;32m      8\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     report_to \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32md:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\transformers\\trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2478\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2479\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2480\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2482\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\transformers\\trainer.py:5153\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5152\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5153\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5155\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\accelerate\\data_loader.py:563\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32md:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32md:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Virtual Environments\\Thesis\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Convert each example into PyTorch tensors\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([ [\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m feature_cols] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m batch ])\n\u001b[0;32m      4\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels}\n",
      "\u001b[1;31mKeyError\u001b[0m: 'electra_0'"
     ]
    }
   ],
   "source": [
    "features = len(combined_dataframe.columns) - 1\n",
    "\n",
    "output_dir = './models/Meta model'\n",
    "model = MLP(input_dim=features , num_labels=4)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate = 0.00002,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    report_to = []\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_dataset[\"train\"],\n",
    "    eval_dataset=combined_dataset[\"test\"],\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "feature_cols = [col for col in combined_dataframe.columns if col != \"label\"]\n",
    "X = combined_dataframe[feature_cols]\n",
    "y = combined_dataframe[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "lr_preds = lr_model.predict(X_test)\n",
    "\n",
    "lr_acc = accuracy_score(y_test, lr_preds)\n",
    "print(\"Logistic Regression Accuracy:\", lr_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.890625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, rf_preds)\n",
    "print(\"Random Forest Accuracy:\", rf_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
